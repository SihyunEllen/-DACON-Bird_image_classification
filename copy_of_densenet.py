# -*- coding: utf-8 -*-
"""Copy of Densenet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18FVLC_Ge16Aa5FVydwnEEjABnuewiv0F
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install einops

pip install --user albumentations

from sklearn import preprocessing

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
from sklearn.model_selection import train_test_split
import numpy as np
from copy import deepcopy # 모델 파라미터값 깊은 복사 (베스트 모델 저장을 위함)
from torch.utils.data import TensorDataset
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from torchvision.transforms import Compose, Resize, ToTensor
from einops import rearrange, reduce, repeat
from einops.layers.torch import Rearrange, Reduce
from torchsummary import summary

import numpy as np
import pandas as pd
import os
from torchvision import transforms
from PIL import Image
from torch.utils.data import Dataset

CFG = {
    'IMG_SIZE': 224,
    'EPOCHS': 5,
    'LEARNING_RATE': 3e-4,
    'BATCH_SIZE': 32,
    'SEED': 41
}
photo_directory = '/content/drive/MyDrive/Colab Notebooks/bird_data'

import random
def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bird_data/train.csv')
train, val, _, _ = train_test_split(df, df['label'], test_size=0.3, stratify=df['label'], random_state=CFG['SEED'])

train

print(train['label'].shape)

train_label_2d = np.array(train['label']).reshape(-1, 1)
val_label_2d = np.array(val['label']).reshape(-1, 1)

from sklearn.preprocessing import OneHotEncoder
oh=OneHotEncoder()

train_onehot= oh.fit_transform(train_label_2d).toarray() ## typeerror로 toarray() 붙여줌
val_onehot = oh.fit_transform(val_label_2d).toarray() ## typeerror로 toarray() 붙여줌

train_label = np.argmax(train_onehot, axis=1)
val_label = np.argmax(val_onehot, axis=1)

train_label.shape

print(train_onehot.shape) #25가지를 원핫으로 바꿈

class CustomDataset(Dataset):
    def __init__(self, imag_path, label_list=None, transform=None):
        self.imag_path = imag_path
        self.label_list = label_list
        self.transform = transform
        if label_list is not None:
            self.label_list = label_list.astype(np.int64)
        self.data = list(zip(imag_path, label_list)) if label_list is not None else list(zip(imag_path))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img_path = photo_directory + self.data[idx][0]
        img_path = img_path.replace('./', '/')
        image = Image.open(img_path)
        if self.transform:
            image = self.transform(image)

        if self.label_list is not None:
            label = self.data[idx][0]
            label = self.label_list[idx]
            return image, label
        else:
            return image

Image.open('/content/drive/MyDrive/Colab Notebooks/bird_data/test/TEST_00000.jpg')

transform = transforms.Compose([
    Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), #imagenet
])

train_dataset = CustomDataset(train['img_path'].values,train_onehot,transform=transform)
train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True)

val_dataset = CustomDataset(val['img_path'].values,val_onehot,transform=transform)
val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True)

print(len(train_loader))  # train_loader에 로드된 데이터의 개수 출력
print(len(val_loader))    # val_loader에 로드된 데이터의 개수 출력

print(val_dataset.label_list.shape)

from torchvision.models import densenet121

model = densenet121(pretrained=True)
num_features = model.classifier.in_features
model.classifier = nn.Linear(num_features, 25)

model

from sklearn.metrics import f1_score

def trainmodel(model, optimizer, train_loader, val_loader, scheduler, device):
    model.to(device)
    criterion = nn.CrossEntropyLoss().to(device)
    # 성능 기록 초기화
    best_score = 0
    best_model = None

    for epoch in range( CFG['EPOCHS']):
        model.train() #모델을 훈련모드로 설정
        train_loss = []

        # 반복을 통해서 배치 단위로 이미지와 라벨을 가져옴
        for imgs, labels in train_loader:
            imgs = imgs.float().to(device)
            labels = labels.float().to(device)

            optimizer.zero_grad()

            output = model(imgs)
            #print(output.shape)
            #print(labels.shape)

            loss = criterion(output, labels)

            loss.backward()
            optimizer.step()

            train_loss.append(loss.item())
        _val_loss, _val_score = validation(model, criterion, val_loader, device)
        _train_loss = np.mean(train_loss) # 각 배치에서 계산된 모든 손실 값의 평균을 구함
        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val F1 Score : [{_val_score:.5f}]')

        # scheduler이 설정되어 있다면 검증 성능에 따라 학습률을 조정
        if scheduler is not None:
            scheduler.step(_val_score)

        # 가장 좋은 성능을 보인 모델을 반환
        if best_score < _val_score:
            best_score = _val_score
            best_model = model

            torch.save(best_model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/Densenet.pth')

    return best_model

def validation(model, criterion, val_loader, device):
    model.eval() # 평가모드
    val_loss = []
    preds, true_labels = [], []

    with torch.no_grad():
        for imgs, labels in val_loader:
            imgs = imgs.float().to(device)
            labels = labels.float().to(device)
            # 데이터 타입 long으로 변경한 후 device로 올림 (int로 변경하였을 때, error 발생했음)

            pred = model(imgs)

            loss = criterion(pred, labels)


            preds += pred.argmax(1).detach().cpu().numpy().tolist()

            true_labels += labels.detach().cpu().numpy().tolist()

            val_loss.append(loss.item())

        _val_loss = np.mean(val_loss)

    return _val_loss, f1_score(true_labels, preds, average='macro')

optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG["LEARNING_RATE"])
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, threshold_mode='abs', min_lr=1e-8, verbose=True)
infer_model = trainmodel(model, optimizer, train_loader, val_loader, scheduler, device)

test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bird_data/test.csv')

infer_model = densenet121(pretrained=True)
infer_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Densenet.pth'))
infer_model.eval()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

test_dataset = CustomDataset(test['img_path'].values, None, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)

infer_model.to(device)

# Generate predictions
predictions = []

with torch.no_grad():
    for images in test_loader:
        images = images.to(device)
        outputs = infer_model(images)
        _, predicted = torch.max(outputs, 1)
        predictions.extend(predicted.cpu().numpy().tolist())

# Create submission DataFrame
submission_df = pd.DataFrame({'id': test['id'], 'label': predictions})

# Save submission file
submission_df.to_csv('/content/drive/MyDrive/Colab Notebooks/bird_data/submission.csv', index=False)

submission_df['label']

classes = train['label'].unique()
print(classes)

def reverse_one_hot_encode(encoded_labels):
    decoded_labels = []
    for encoded_label in encoded_labels:
        label=oh.inverse_transform(encoded_label)
        decoded_labels.append(label)
    return decoded_labels

decoded_predictions = reverse_one_hot_encode(predictions)

submission_df = pd.DataFrame({'id': test['id'], 'label': decoded_predictions})
submission_df.to_csv('/content/drive/MyDrive/Colab Notebooks/bird_data/sample_submission.csv', index=False)